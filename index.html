<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Naveen Madapana</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<!-- This is<br /> -->
						<h1>Naveen Madapana</h1>
						<p> I am a Ph.D. candidate working with <a href="https://web.ics.purdue.edu/~jpwachs/">Dr. Juan Wachs </a> in the <a href="https://engineering.purdue.edu/isat/">ISAT</a> lab at Purdue University. My research interests include human-computer interaction, AI for health care applications, and machine learning. Outside work, I love to cook, run, play basketball, hike and travel.
						</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Naveen Madapana</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Research</a></li>
							<li><a href="publications.html">Publications</a></li>
							<li><a href="blog.html">Blog</a></li>
							<li><a href="news.html">News</a></li>
							<li><a href="contact.html">Contact</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/nmadapan/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/nmadapan" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://scholar.google.com/citations?user=X6gXGR4AAAAJ&hl=en&oi=ao" class="icon brands fa-google"><span class="label">Google</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
									<span class="date">April 2021</span>
									<h2><a href="#">Human-Like Learning Systems</a></h2>
									<p>There are significant differences between the way machines and humans represent knowledge, assimilate and learn new concepts. For instance, machines require hundreds of examples to learn to classify, however, humans can efficiently discriminate categories just by looking at few examples of them or solely based on semantic information. The next generation learning techniques such as few-shot, one-shot and zero-shot learning will aid this ultimate goal of building machines that can continually learn like humans. </p>
								</header>
								<a href="#" class="image main"><img src="images/long-tail.png" alt="" /></a>
								<ul class="actions special">
									<li><a href="#" class="button large">Full Story</a></li>
								</ul>
							</article>

						<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										<span class="date">October 2020</span>
										<h2><a href="https://openaccess.thecvf.com/content/WACV2021/papers/Xiao_One-Shot_Image_Recognition_Using_Prototypical_Encoders_With_Reduced_Hubness_WACV_2021_paper.pdf">One-Shot <br />
										Prototypical Encoders</a></h2>
									</header>
									<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Xiao_One-Shot_Image_Recognition_Using_Prototypical_Encoders_With_Reduced_Hubness_WACV_2021_paper.pdf" class="image fit"><img src="images/wacv-vpe-pp.png" alt="" /></a>
									<p>This work proposes to learn an image-to-image translation task in which the goal is to predict the class prototypes from raw images. This approach regulates the latent space by inherently reducing data hubness and it further incorporates contrastive and multi-task losses to increase the discriminative ability of few-shot models.</p>
<!-- 									<ul class="actions special">
										<li><a href="https://openaccess.thecvf.com/content/WACV2021/papers/Xiao_One-Shot_Image_Recognition_Using_Prototypical_Encoders_With_Reduced_Hubness_WACV_2021_paper.pdf" class="button">Paper</a></li>
									</ul> -->
								</article>
								<article>
									<header>
										<span class="date">September 2020</span>
										<h2><a href="https://ieeexplore.ieee.org/abstract/document/9320217">Feature Selection<br />
										Zero-Shot Learning</a></h2>
									</header>
									<a href="https://ieeexplore.ieee.org/abstract/document/9320217" class="image fit"><img src="images/fg2020_feature_selection.png" alt="" /></a>
									<p>This project studies the effectiviness of three kinds of features: 1. heuristic-based, 2. raw velocity and 3. deep-network-based features in the context of zero-shot learning for gesture recognition. In addition, this work proposes a linear and bi-linear model that jointly optimizes for semantic and classification losses. </p>
<!-- 									<ul class="actions special">
										<li><a href="#" class="button">Full Story</a></li>
									</ul> -->
								</article>
								<article>
									<header>
										<span class="date">June 2020</span>
										<h2><a href="https://ieeexplore.ieee.org/abstract/document/9104742">Agreement Analysis<br />
										Gesture Semantic Vectors</a></h2>
									</header>
									<a href="https://ieeexplore.ieee.org/abstract/document/9104742" class="image fit"><img src="images/ex-soft-repr.png" alt="" /></a>
									<p>A general framework that inherently incorporates gesture descriptors into the agreement analysis. A new metric referred to as soft agreement rate (SAR) to measure the level of agreement was proposed. Our computational experiments to demonstrate that existing agreement metrics are a special case of our approach.</p>
<!-- 									<ul class="actions special">
										<li><a href="#" class="button">Full Story</a></li>
									</ul> -->
								</article>
								<article>
									<header>
										<span class="date">April 14, 2017</span>
										<h2><a href="#">Touchless Interface For<br />
										Opearting Room </a></h2>
									</header>
									<a href="#" class="image fit"><img src="images/surgeon_gest_speech.png" alt="" /></a>
									<p>This project lasted for over four years and is funded by the Agency of Healthcare Research and Quality. The main goal is to reduce the touch-based infections by building a touchless system powered by speech and gestures in the OR. User experiments were conducted with surgeons to know their preferneces and to test the system. <a href="#">Read more</a> </p>
<!-- 									<ul class="actions special">
										<li><a href="#" class="button">Full Story</a></li>
									</ul> -->
								</article>
								<article>
									<header>
										<span class="date">May 2020</span>
										<h2><a href="https://ieeexplore.ieee.org/abstract/document/8756548">ZSL Dataset<br />
										Gesture Recognition</a></h2>
									</header>
									<a href="https://ieeexplore.ieee.org/abstract/document/8756548" class="image fit"><img src="images/gesture-big-picture-attributes3.png" alt="" /></a>
									<p>This work presents the first annotated database of attributes for the categories present in ChaLearn 2013 and MSRC-12 datasets. We relied on literature in semantic and computational linguistics, and crowdsourced annotation platforms such as Amazon Mechanical Turk to build this attribute-based dataset for gestures. </p>
<!-- 									<ul class="actions special">
										<li><a href="#" class="button">Full Story</a></li>
									</ul> -->
								</article>
								<article>
									<header>
										<span class="date">October 2019</span>
										<h2><a href="https://arxiv.org/pdf/1903.00959.pdf">DESK<br />
										Robitics Activity Dataset</a></h2>
									</header>
									<a href="https://arxiv.org/pdf/1903.00959.pdf" class="image fit"><img src="images/desk.png" alt="" /></a>
									<p>DESK (Dexterous Surgical Skill) dataset comprises a set of surgical robotic skills collected during a surgical training task using three robotic platforms: the Taurus II robot, Taurus II simulated robot, and the YuMi robot. This dataset was used to test the idea of transferring knowledge across different domains (eg from Taurus to YuMi robot) for a surgical gesture classification task. </p>
<!-- 									<ul class="actions special">
										<li><a href="#" class="button">Full Story</a></li>
									</ul> -->
								</article>
							</section>

					</div>

				<!-- Copyright -->
					<!-- <div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div> -->

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
